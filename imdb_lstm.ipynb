{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIZENBcOU7uR771YGMy91l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisestella/deepLearning/blob/main/imdb_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vwj6J3cZRhRa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import argparse, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import models, layers, optimizers, losses, metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants and hyperparameters\n",
        "MAX_WORD_INDEX = 10000\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 20\n",
        "LR = 0.001\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.999\n",
        "EPSILON = 1.0e-8\n",
        "DECAY = 0.0\n",
        "VAL_PERC = 0.4\n",
        "\n",
        "EMBEDDING_DIM = 64\n",
        "NUM_LSTM_UNITS = 32\n",
        "DROPOUT_RATE = 0.5"
      ],
      "metadata": {
        "id": "eTABrUhsR4GC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load database using Keras\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = MAX_WORD_INDEX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wFf9_zMR5WX",
        "outputId": "f5585827-a29a-403f-91f7-8d264f541951"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#  print some information on the data\n",
        "max_seq_len_train = max([len(sequence) for sequence in train_data])\n",
        "max_seq_len_test = max([len(sequence) for sequence in test_data])\n",
        "min_seq_len_train = min([len(sequence) for sequence in train_data])\n",
        "min_seq_len_test = min([len(sequence) for sequence in test_data])\n",
        "print(f'Maximum train sequence length: {max_seq_len_train}')\n",
        "print(f'Maximum test sequence length: {max_seq_len_test}')\n",
        "print(f'Minimum train sequence length: {min_seq_len_train}')\n",
        "print(f'Minimum test sequence length: {min_seq_len_test}')\n"
      ],
      "metadata": {
        "id": "EZK8QFmjSDUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly selects a sentence, look at the encoding and check its label\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "ind = 33\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[ind]])\n",
        "\n",
        "print(f'REVIEW:\\n {decoded_review}\\n')\n",
        "print(f'Encoded sequence of words:\\n {train_data[ind]}\\n')\n",
        "print(f'Label: {train_labels[ind]}\\n')"
      ],
      "metadata": {
        "id": "kcWXWdbeSOhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# pad sequences\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(train_data)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(test_data)\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "\n"
      ],
      "metadata": {
        "id": "WHpfsaACSPq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# transform labels  into arrays\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test  = np.asarray(test_labels).astype('float32')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')\n",
        "\n"
      ],
      "metadata": {
        "id": "M1k1aIORSSAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# split training data into training and validation\n",
        "nsamples = X_train.shape[0]\n",
        "nval_samples = int(VAL_PERC * nsamples)\n",
        "X_val = X_train[:nval_samples]\n",
        "partial_X_train = X_train[nval_samples:]\n",
        "y_val = y_train[:nval_samples]\n",
        "partial_y_train = y_train[nval_samples:]\n",
        "\n"
      ],
      "metadata": {
        "id": "w_VV7qKMSV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# build model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(MAX_WORD_INDEX, EMBEDDING_DIM))\n",
        "model.add(layers.LSTM(\n",
        "    units=NUM_LSTM_UNITS,\n",
        "    ))\n",
        "model.add(layers.Dropout(rate=DROPOUT_RATE))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "\n"
      ],
      "metadata": {
        "id": "hAF9IiwtSY06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# set optimizer\n",
        "opt = optimizers.Adam(lr=LR,\n",
        "                      beta_1=BETA1,\n",
        "                      beta_2=BETA2,\n",
        "                      epsilon=EPSILON,\n",
        "                      decay=DECAY)\n",
        "\n",
        "# set loss and metrics\n",
        "loss = losses.binary_crossentropy\n",
        "met = [metrics.binary_accuracy]\n",
        "\n",
        "# compile model: optimization method, training criterion and metrics\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=loss,\n",
        "    metrics=met\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZdvXyl7SbfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# early stop\n",
        "callbacks_list = [\n",
        "    EarlyStopping(\n",
        "        monitor='binary_accuracy',\n",
        "        patience=10),\n",
        "    ]\n",
        "\n"
      ],
      "metadata": {
        "id": "WA8NLdemSdjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# train model\n",
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    shuffle=True,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=callbacks_list,\n",
        "                    verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "QZTJ1fgYSq4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning curves\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "# losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "# accuracies\n",
        "acc_values = history_dict['binary_accuracy']\n",
        "val_acc_values = history_dict['val_binary_accuracy']\n",
        "\n",
        "epochs = range(NUM_EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,8))\n",
        "\n",
        "ax1.plot(epochs, loss_values, 'bo', label=\"Training Loss\")\n",
        "ax1.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss Value')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(epochs, acc_values, 'ro', label=\"Training Accuracy\")\n",
        "ax2.plot(epochs, val_acc_values, 'r', label=\"Validation Accuracy\")\n",
        "ax2.set_title('Training and Validation Accuraccy')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1uxQjWtS3Yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}